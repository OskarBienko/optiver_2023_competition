{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"dockerImageVersionId":30553,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom statsmodels.api import Logit, OLS\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import acf, pacf\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom typing import List, Dict\n\n# Modeling\nfrom sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error\nfrom sklearn.model_selection import (\n    TimeSeriesSplit, KFold, cross_val_score,\n)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport sklearn.tree\nimport sklearn.ensemble\nimport sklearn.linear_model\n\nimport xgboost as xgb\nimport lightgbm as lgbm","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:04:16.568375Z","iopub.execute_input":"2023-12-04T09:04:16.568732Z","iopub.status.idle":"2023-12-04T09:04:23.199072Z","shell.execute_reply.started":"2023-12-04T09:04:16.568700Z","shell.execute_reply":"2023-12-04T09:04:23.197938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def perform_adf_test(series: pd.Series = None) -> float:\n   '''Perform Augmented Dickey Fuller test\n\n   :param series: time series, defaults to None\n   :type series: pd.Series, optional\n   :return: pvalue\n   :rtype: float\n    \n   Stationarity means that the statistical properties of a time series, i.e. mean, variance and covariance do not change over time. \n   Many statistical models require the series to be stationary to make effective and precise predictions.\n   The null hypothesis of the Augmented Dickey-Fuller is that there is a unit root (series is not stationary).\n   The alternative hypothesis is that there is no unit root.'\n   '''\n   pval = np.round(adfuller(x=series, regression='c', autolag=None)[1], 3)\n   return pval","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:04:23.201015Z","iopub.execute_input":"2023-12-04T09:04:23.201381Z","iopub.status.idle":"2023-12-04T09:04:23.208347Z","shell.execute_reply.started":"2023-12-04T09:04:23.201350Z","shell.execute_reply":"2023-12-04T09:04:23.207089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_pacf(series:pd.Series, series_name:str, nlags:int, partial:bool=True, save:bool=False):\n    \n    '''\n    This func estimates the (partial) autocorrelation function of a given time series using statsmodels package and then plots it using plotly package\n    Note that the default pacf calculation method is Yule-Walker with sample size-adjustment in denominator for acovf\n    Note that the default confidence interval is 95%\n    \n    :param series: observations of time series for which acf/pacf is calculated\n    :type series: pd.Series\n    :param series_name: at least the name and the frequency of the time series, without special characters, that will be used to create a title\n    :type series_name: str\n    :param nlags: number of lags to return autocorrelation for\n    :type nlags: int\n    :param partial: whether to plot pacf\n    :type partial: bool (default True)\n    :param save: whether to save the plot to the current workding directory\n    :type save: bool (default False)\n    '''\n    \n    if not isinstance(series, pd.Series):\n        raise Exception('Time series is not a pd.Series type!')\n\n    # Define the title depending on the bool argument\n    title=f'PACF of {series_name}' if partial else f'ACF of {series_name}'\n    \n    # Calculate the acf/pacf and the confidence intervals\n    corr_array, conf_int_array = pacf(series.dropna(), alpha=0.05, nlags=nlags, method='yw') if partial else acf(series.dropna(), alpha=0.05, nlags=nlags)\n    \n    # Center the confidence intervals so that it's easy to visually inspect if a given correlation is significantly different from zero\n    lower_y = conf_int_array[:,0] - corr_array\n    upper_y = conf_int_array[:,1] - corr_array\n    \n    # Create an empty figure\n    fig = go.Figure()\n\n    # Plot the correlations using vertical lines\n    [fig.add_scatter(x=(x,x), y=(0,corr_array[x]), mode='lines', line_color='#3f3f3f', hoverinfo='skip') \n        for x in np.arange(len(corr_array))]\n    \n    # Plot the correlations using markers\n    # The <extra></extra> part removes the trace name\n    fig.add_scatter(\n        x=np.arange(len(corr_array)),\n        y=corr_array,\n        mode='markers',\n        marker_color='#1f77b4',\n        marker_size=12,\n        hovertemplate=\n        'Lag %{x}<br>' +\n        'Corr: %{y:.2f}<br>' +\n        '<extra></extra>'\n    )\n    \n    # Plot the centered confidence intervals\n    fig.add_scatter(x=np.arange(len(corr_array)), y=upper_y, mode='lines', line_color='rgba(255,255,255,0)', hoverinfo='skip')\n    fig.add_scatter(x=np.arange(len(corr_array)), y=lower_y, mode='lines', fillcolor='rgba(32, 146, 230,0.3)',\n        fill='tonexty', line_color='rgba(255,255,255,0)', hoverinfo='skip')\n    \n    # Prettify the plot\n    fig.update_traces(showlegend=False)\n    fig.update_xaxes(tickvals=np.arange(start=0, stop=nlags+1))\n    fig.update_yaxes(zerolinecolor='#000000')\n    fig.update_layout(title=title, title_x=0.5, width=500, height=300, margin=dict(l=0, r=0, b=0, t=30, pad=1))\n    # fig.update_layout(title=title, title_x=0.5, width=500, height=300, hovermode=False, margin=dict(l=0, r=0, b=0, t=30, pad=1))\n\n    # Save the plot to the current working directory\n    if save:\n        fig.write_image(f'''{title.replace(' ', '_')}.png''')\n\n    # Eventually show the plot\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:04:30.137486Z","iopub.execute_input":"2023-12-04T09:04:30.137903Z","iopub.status.idle":"2023-12-04T09:04:30.153887Z","shell.execute_reply.started":"2023-12-04T09:04:30.137874Z","shell.execute_reply":"2023-12-04T09:04:30.152985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inspect_columns(df):\n    \n    # Get this function from other's Jupyter\n    # A helper function that does a better job than df.info() and df.describe()\n    \n    result = pd.DataFrame({\n        'unique': df.nunique() == len(df),\n        'cardinality': df.nunique(),\n        'with_null': df.isna().any(),\n        'null_pct': round((df.isnull().sum() / len(df)) * 100, 2),\n        '1st_row': df.iloc[0],\n        'random_row': df.iloc[np.random.randint(low=0, high=len(df))],\n        'last_row': df.iloc[-1],\n        'dtype': df.dtypes\n    })\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:04:30.989822Z","iopub.execute_input":"2023-12-04T09:04:30.990562Z","iopub.status.idle":"2023-12-04T09:04:30.997302Z","shell.execute_reply.started":"2023-12-04T09:04:30.990510Z","shell.execute_reply":"2023-12-04T09:04:30.996330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\n\nlogging.basicConfig(level=logging.INFO)  # Set the logging level\nlogger = logging.getLogger(__name__)  # Create a logger\n\ndef reduce_mem_usage(df: pd.DataFrame=None, verbose: int=0) -> pd.DataFrame:\n    \n    \"\"\"\n    The function modifies the input DataFrame in place \n    by changing the data types of numeric columns to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    timedelta_cols = df.select_dtypes(include=[np.timedelta64]).columns\n    \n    # Create a list of valid numeric columns by excluding timedelta columns\n    valid_cols = list(set(numeric_cols) - set(timedelta_cols))\n    valid_cols = [col for col in valid_cols if col != 'stock_id']\n\n    for col in valid_cols:        \n        col_type = df[col].dtype\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            # For integer columns:\n                # Check if the column values fit within the range of a smaller integer type (e.g., int8, int16, int32, int64)\n                # Downcast the column to the smallest integer type that can safely store the values\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            \n            # For float columns:\n                # Check if the column values fit within the range of a smaller float type (e.g., float16, float32)\n                # Downcast the column to the smallest float type that can safely store the values\n            else:\n\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n    \n    logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n    end_mem = df.memory_usage().sum() / 1024**2\n    logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n    decrease = 100 * (start_mem - end_mem) / start_mem\n    logger.info(f\"Decreased by {decrease:.2f}%\")\n    \n    # In Kaggle notebooks logger() statements aren't displayed\n    print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n    print(f\"Decreased by {decrease:.2f}%\")\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:04:32.350347Z","iopub.execute_input":"2023-12-04T09:04:32.350772Z","iopub.status.idle":"2023-12-04T09:04:32.370000Z","shell.execute_reply.started":"2023-12-04T09:04:32.350740Z","shell.execute_reply":"2023-12-04T09:04:32.368621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_mae(y_true, y_pred):\n    # Calculate the absolute errors\n    errors = np.abs(y_true - y_pred)\n    \n    # Compute the gradient of the MAE\n    grad = np.where(y_true > y_pred, -1, 1)\n    \n    # Hessian is a constant value for MAE\n    hess = np.ones_like(y_true)\n    \n    # Calculate the mean absolute error\n    objective = np.mean(errors)\n    \n    return grad, hess\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:04:33.877531Z","iopub.execute_input":"2023-12-04T09:04:33.877921Z","iopub.status.idle":"2023-12-04T09:04:33.885041Z","shell.execute_reply.started":"2023-12-04T09:04:33.877892Z","shell.execute_reply":"2023-12-04T09:04:33.883868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***WAP - Weighted Average Price***\n\n\n- **Weighted Averaged Price (WAP)**\n\nWAP in a nut shell, it's a price take bid/ask size into consideration. A fair book-based valuation must take two factors into account: the level and the size of orders.\n\n$$ WAP = \\frac{BidPrice_{1} * AskSize_{1} + AskPrice_{1} * BidSize_{1}}{BidSize_{1}+AskSize_{1} } $$\n\nAs you can see, if two books have both bid and ask offers on the same price level respectively, the one with more offers in place will generate a lower stock valuation, as there are more intended seller in the book. On the other hand, more seller implies a fact of more supply on the market resulting in a lower stock valuation.\n\nThe idea behind this weighting is that higher buying pressure tends to drive prices up, while an increase in sellers tends to push prices down.\nTo illustrate this idea with an extreme example: consider a specific moment where the best bid is 5 with a BidSize of 5000, and the best ask is 6 with an AskSize of 10. In this scenario, the number of buyers is way larger than the number of sellers, suggesting that the price should be very close to 6. The WAP reflects this intuition, as it gives more weight to the ask price.\n\n- **Let's take a look at the following 2 scenarios:**\n\n- **Scenario 1: More Buyer, more bid szie, therefore higher WAP**\n\n        - bid price = 1, ask price =2, with ask size = 1, bid size = 2\n\n$$WAP_{1} = \\frac{1 * 1 + 2 * 2}{3}  = \\frac{5}{3}$$\n\n- **Scenario 2: More Seller, more ask size,  therefore lower WAP**\n\n        - bid price = 1, ask price =2, with ask size = 2, bid size = 1\n\n\n$$ WAP_{2} = \\frac{1 * 2 + 2 * 1}{3}  = \\frac{4}{3}$$\n\n\nRemark: Note that in most of cases, during the continuous trading hours, an order book should not have the scenario when bid order is higher than the offer, or ask, order. In another word, most likely, the bid and ask should never be in cross.","metadata":{"execution":{"iopub.status.busy":"2023-10-14T11:48:26.297047Z","iopub.execute_input":"2023-10-14T11:48:26.297427Z","iopub.status.idle":"2023-10-14T11:48:26.308678Z","shell.execute_reply.started":"2023-10-14T11:48:26.297402Z","shell.execute_reply":"2023-10-14T11:48:26.307198Z"}}},{"cell_type":"markdown","source":"# ***Target Variable***\n\n- **What is Target**: \n\n    The 60 second future move in the wap of the stock, less the 60 second future move of the synthetic index. Only provided for the train set.\n\n- **Target Variable Formula**:\n\n$$ Target = (\\frac{WAP_{t+60}}{WAP_{t}} - \\frac{Index_{t+60}}{Index_{t}})$$\n\nTherefore, I think we can only use the stock information to predict the target\n\n\n- **Remark**:\n\n    1) The benchmark synthetic index  = The synthetic index is a custom weighted index of Nasdaq-listed stocks constructed by Optiver for this competition.\n\n    2) The unit of the target is basis points, which is a common unit of measurement in financial markets.  A 1 bp price move is equivalent to a 0.01% price move.","metadata":{}},{"cell_type":"markdown","source":"# ***Closing Auction Order Book Data***\n\n\n\n- **What is closing auction:** \n\n    - 1) **What is an auction?** Auction is a period of time that lets multiple buyers and sellers trade in a single price and in a single joint transaction. There are many different kinds of auction, for example London Stock Exchange offers **Opening Auction**, **Intra-Day Auction**, and **Closing Auction**. In this competition teams are particularly interested in the type of a **closing auction**.\n    \n    - 2) Closing auction starts after continuous trading (Market Open session)\n    \n         - Take London Stock Exchange for example: \n   \n            (2-1) Opening Auction Session: 7:50 pm - 8:00 pm  \n            (2-2) Intra-Day Auction Session: 12:00 am - 12:02 am  \n            (2-3) Closing Auction Session: 16:30 am - 16:35 am  \n            (2-4) Rest of the time is regular trading hour or continous trading hour\n          \n    - 3) **The closing auction** trading period is a trading mechanism with a single price bidding. Buyers and sellers enter buy and sell orders during this period. This period is also known as **call period**. After collecting the relevant orders, the trading system will determine the order that can complete the most transactions based on the interaction of the orders. This event is known as **uncrossing event**.\n\n    - 4) **Why do we care about the closing auction book?** In the last ten minutes of the Nasdaq exchange trading session, market makers like Optiver merge traditional **order book data** with **auction book data(Closing Auction Order Book Data)**. This ability to consolidate information from both sources is critical for providing the best prices to all market participants.\n    \n    - 5) Bid & ask price of the closing auction book dataset in the call period are now **overlapping**.\n    \n    - 6) A typical closing auction book data **before** uncrossing event might look like this:\n\n\n| Bid Size (Buyer)| Price | Ask Size (Seller) |\n| --- | --- | --- |\n|  | 10 | 1 |\n| 3 | 9 | 2 |\n| 4 | 8 | 4 |","metadata":{}},{"cell_type":"markdown","source":"- **Example of closig auction mechanism:** \n\n    For the closing auction book example above, we can work through definitions for a few key terms. Suppose the auction uncrossed with the book in this state, then:\n\n    - 1) At a price of 10, 0 lots would be matched since there as no bids >= 10.\n\n    - 2) At a price of 9, 3 lots would be matched, as there are 3 bids >=9 and 6 asks <= 9.\n\n    - 3) At a price of 8, 4 lots would be matched, since are 7 bids>=8, and there are 4 asks<=8.\n\n        - The price which maximises the number of matched lots would be 8, therefore 8 is our uncross price.  \n    \n    \n- **Key Words Terminology with closing auction order book data**\n\n    - 1) **Uncross Price**: The price which maximises the number of matched lots. In the above example will be 8.\n\n    - 2) **Matched Size**: the max number of matched lots by one single price. In the above example will be 4.\n\n    - 3) **Imbalance**: Imbalance refers to the number of unmatched shares. In the above example, the uncross price is 8 and there are 7 bids & 4 asks which can be matched for the price of 8. As a result, we have three unmatched bids remaining, and the **remaining bids** contribute to an imbalance of three lots in the **buy direction**.\n\n- **More Key Words Terminology for Price Explanation:**\n\n    - 1) **Far Price**: The crossing price that will maximize the number of shares matched based on auction interest only. Therfore, Far price refers to the hypothetical uncross price of the auction book, if it were to uncross at the reporting time.\n\n    - 2) **Near Price**: The crossing price that will maximize the number of shares matched **based auction** and **continuous market orders**. Nasdaq provides near price information 5 minutes before the closing cross. Therefore, in the below figure we can see that starting from 300 sec, the near price/far price start to show.\n\n    - 3) **Reference Price**: \n\n        - If the \"Near Price\" is between the best bid and ask, then the reference price is equal to the near price\n\n        - If the \"Near Price\" > best ask, then reference price = best ask\n\n        - If the \"Near Price\" < best bid, then reference price = best bid So the reference price is the near price bounded between the best bid and ask. **In summary for reference price, it's a uncross price that best represent the price that will maximize the match lots in the order book data.**","metadata":{}},{"cell_type":"markdown","source":"# ***Imbalance Buy/Sell Flag***\n\n**- Imbalance** refers to the number of unmatched shares.\n\n| Bid Size (Buyer)| Price | Ask Size (Seller) |\n| --- | --- | --- |\n|  | 10 | 1 |\n| 3 | 9 | 2 |\n| 4 | 8 | 4 |\n\n\nIn the above example, the uncross price is 8 and there are 7 bids & 4 asks which can be matched, therefore we are left with 3 bids unmatched. the left over are bid size, therefore, there is an imbalance of 3 lots in the buy direction.\n\n- Imbalance_size: The amount unmatched at the current reference price (in USD).\n\n- Imbalance_buy_sell_flag: An indicator reflecting the direction of auction imbalance.\n\n        - buy-side imbalance: 1\n        - sell-side imbalance: -1\n        - no imbalance: 0\n        \n        \n - Remark: during my EDA process, I found that \"imbalance_buy_sell_flag * imbalance_size\" better reflect the information of bid/ask size.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:04:36.847889Z","iopub.execute_input":"2023-12-04T09:04:36.848257Z","iopub.status.idle":"2023-12-04T09:04:57.485247Z","shell.execute_reply.started":"2023-12-04T09:04:36.848229Z","shell.execute_reply":"2023-12-04T09:04:57.483815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inspect_columns(df)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:04:57.489632Z","iopub.execute_input":"2023-12-04T09:04:57.490431Z","iopub.status.idle":"2023-12-04T09:05:09.702123Z","shell.execute_reply.started":"2023-12-04T09:04:57.490397Z","shell.execute_reply":"2023-12-04T09:05:09.700606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seconds_in_bucket ends at 540, which makes perfect sense\npx.line(df['seconds_in_bucket'].unique(), markers='.')","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:09.703922Z","iopub.execute_input":"2023-12-04T09:05:09.704364Z","iopub.status.idle":"2023-12-04T09:05:11.674484Z","shell.execute_reply.started":"2023-12-04T09:05:09.704325Z","shell.execute_reply":"2023-12-04T09:05:11.673386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Numbers of stocks per date in time has benn increasing\npx.line(df.groupby(by=df['date_id'])['stock_id'].nunique(), markers='.')","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:11.677521Z","iopub.execute_input":"2023-12-04T09:05:11.678183Z","iopub.status.idle":"2023-12-04T09:05:12.428358Z","shell.execute_reply.started":"2023-12-04T09:05:11.678151Z","shell.execute_reply":"2023-12-04T09:05:12.427237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take a look at what a single session might like\ndf.loc[(df['stock_id'] == 0) & (df['date_id'] == 0)]","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:12.430337Z","iopub.execute_input":"2023-12-04T09:05:12.431172Z","iopub.status.idle":"2023-12-04T09:05:12.511525Z","shell.execute_reply.started":"2023-12-04T09:05:12.431133Z","shell.execute_reply":"2023-12-04T09:05:12.510325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adf_pvalues = np.empty(df['stock_id'].nunique())\nfor i, stock in tqdm(enumerate(df['stock_id'].unique())):\n    adf_pvalues[i] = perform_adf_test(df.loc[df['stock_id'] == stock, 'target'].dropna())","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:12.512749Z","iopub.execute_input":"2023-12-04T09:05:12.513055Z","iopub.status.idle":"2023-12-04T09:05:46.456049Z","shell.execute_reply.started":"2023-12-04T09:05:12.513022Z","shell.execute_reply":"2023-12-04T09:05:46.454886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target is stationary for each of the stock\nadf_pvalues[adf_pvalues > 0]","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:46.457780Z","iopub.execute_input":"2023-12-04T09:05:46.458555Z","iopub.status.idle":"2023-12-04T09:05:46.467006Z","shell.execute_reply.started":"2023-12-04T09:05:46.458494Z","shell.execute_reply":"2023-12-04T09:05:46.466056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target is highly correlated with itself for the same stock\nstock = 0\nplot_pacf(series=df.loc[df['stock_id'] == stock, 'target'].dropna(), series_name=f'stock_id = {stock}', nlags=50, partial=False)\nplot_pacf(series=df.loc[df['stock_id'] == stock, 'target'].dropna(), series_name=f'stock_id = {stock}', nlags=50)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:46.468427Z","iopub.execute_input":"2023-12-04T09:05:46.469166Z","iopub.status.idle":"2023-12-04T09:05:46.955297Z","shell.execute_reply.started":"2023-12-04T09:05:46.469127Z","shell.execute_reply":"2023-12-04T09:05:46.954129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target is highly correlated with itself for the same stock\nstock = 100\nplot_pacf(series=df.loc[df['stock_id'] == stock, 'target'].dropna(), series_name=f'stock_id = {stock}', nlags=50, partial=False)\nplot_pacf(series=df.loc[df['stock_id'] == stock, 'target'].dropna(), series_name=f'stock_id = {stock}', nlags=50)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:46.956571Z","iopub.execute_input":"2023-12-04T09:05:46.956887Z","iopub.status.idle":"2023-12-04T09:05:47.285773Z","shell.execute_reply.started":"2023-12-04T09:05:46.956860Z","shell.execute_reply":"2023-12-04T09:05:47.284696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target exhibits some autocorrelation that I could take advantage of\n# Since target is wap(t+60) / wap, I could use target.shift(7) as a feature\n# Nevertheless, the model should be stock-agnostic, since the organizers highlighted\n# that the number of different stocks may change in time\n# So probably they will include a couple of new stocks to the test set ","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:47.289581Z","iopub.execute_input":"2023-12-04T09:05:47.289917Z","iopub.status.idle":"2023-12-04T09:05:47.294994Z","shell.execute_reply.started":"2023-12-04T09:05:47.289889Z","shell.execute_reply":"2023-12-04T09:05:47.293912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[(df['stock_id'] == 0) & (df['date_id'] == 7)].corr(method='spearman').sort_values(by=['target']).round(2)[['target']]","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:47.296680Z","iopub.execute_input":"2023-12-04T09:05:47.297413Z","iopub.status.idle":"2023-12-04T09:05:47.336937Z","shell.execute_reply.started":"2023-12-04T09:05:47.297373Z","shell.execute_reply":"2023-12-04T09:05:47.335980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.line(df.loc[(df['seconds_in_bucket'] == 0), 'target'])","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:47.338418Z","iopub.execute_input":"2023-12-04T09:05:47.338754Z","iopub.status.idle":"2023-12-04T09:05:47.496044Z","shell.execute_reply.started":"2023-12-04T09:05:47.338720Z","shell.execute_reply":"2023-12-04T09:05:47.494121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There is no autocorrelation for a single time period, so that I can iterate over 'date', `each seconds_in_bucket` values\n# I don't need to be aware of data leakage\nplot_pacf(series=df.loc[df['seconds_in_bucket'] == 0, 'target'].dropna(), series_name=f'seconds_in_bucket = {0}', nlags=100, partial=False)\nplot_pacf(series=df.loc[df['seconds_in_bucket'] == 0, 'target'].dropna(), series_name=f'seconds_in_bucket = {0}', nlags=100)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:47.497187Z","iopub.execute_input":"2023-12-04T09:05:47.497576Z","iopub.status.idle":"2023-12-04T09:05:49.145806Z","shell.execute_reply.started":"2023-12-04T09:05:47.497518Z","shell.execute_reply":"2023-12-04T09:05:49.144720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:49.147213Z","iopub.execute_input":"2023-12-04T09:05:49.148271Z","iopub.status.idle":"2023-12-04T09:05:49.156468Z","shell.execute_reply.started":"2023-12-04T09:05:49.148231Z","shell.execute_reply":"2023-12-04T09:05:49.155446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:49.158093Z","iopub.execute_input":"2023-12-04T09:05:49.158849Z","iopub.status.idle":"2023-12-04T09:05:49.195584Z","shell.execute_reply.started":"2023-12-04T09:05:49.158812Z","shell.execute_reply":"2023-12-04T09:05:49.194468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['imbalance_size_vs_matched_size'] = df['imbalance_size'] / df['matched_size']\ndf['imbalance_size_vs_matched_size*imbalance_buy_sell_flag'] = df['imbalance_size_vs_matched_size'] * df['imbalance_buy_sell_flag']\ndf['bid_ask_spread_relative'] = (df['bid_price'] - df['ask_price']) / df['ask_price']\n\ndf[['far_price', 'near_price']] = df[['far_price', 'near_price']].fillna(0)\ndf['near_price_vs_reference_price'] = df['near_price'] / df['reference_price']\ndf['far_price_vs_reference_price'] = df['far_price'] / df['reference_price']\ndf['bid_price_vs_reference_price'] = df['bid_price'] / df['reference_price']\ndf['ask_price_vs_reference_price'] = df['ask_price'] / df['reference_price']","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:49.197434Z","iopub.execute_input":"2023-12-04T09:05:49.198117Z","iopub.status.idle":"2023-12-04T09:05:49.440232Z","shell.execute_reply.started":"2023-12-04T09:05:49.198079Z","shell.execute_reply":"2023-12-04T09:05:49.439169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define features\nfeatures = [\n    'imbalance_size_vs_matched_size', 'imbalance_size_vs_matched_size*imbalance_buy_sell_flag', 'bid_ask_spread_relative',\n    'near_price_vs_reference_price', 'far_price_vs_reference_price',\n    'bid_price_vs_reference_price', 'ask_price_vs_reference_price',\n]","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:49.441422Z","iopub.execute_input":"2023-12-04T09:05:49.441724Z","iopub.status.idle":"2023-12-04T09:05:49.446598Z","shell.execute_reply.started":"2023-12-04T09:05:49.441699Z","shell.execute_reply":"2023-12-04T09:05:49.445717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[features].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:49.448221Z","iopub.execute_input":"2023-12-04T09:05:49.448609Z","iopub.status.idle":"2023-12-04T09:05:49.655663Z","shell.execute_reply.started":"2023-12-04T09:05:49.448569Z","shell.execute_reply":"2023-12-04T09:05:49.654692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[df['ask_price_vs_reference_price'].isna()].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:49.656792Z","iopub.execute_input":"2023-12-04T09:05:49.657077Z","iopub.status.idle":"2023-12-04T09:05:49.674773Z","shell.execute_reply.started":"2023-12-04T09:05:49.657053Z","shell.execute_reply":"2023-12-04T09:05:49.673691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# These are certainly uninformative rows, drop them without any second thoughts\ndf = df.loc[~df['ask_price_vs_reference_price'].isna()].copy()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:49.676211Z","iopub.execute_input":"2023-12-04T09:05:49.676637Z","iopub.status.idle":"2023-12-04T09:05:51.639014Z","shell.execute_reply.started":"2023-12-04T09:05:49.676588Z","shell.execute_reply":"2023-12-04T09:05:51.637559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = reduce_mem_usage(df)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:51.640698Z","iopub.execute_input":"2023-12-04T09:05:51.641280Z","iopub.status.idle":"2023-12-04T09:05:52.883259Z","shell.execute_reply.started":"2023-12-04T09:05:51.641249Z","shell.execute_reply":"2023-12-04T09:05:52.882125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IDEAS\n# Attempt 1: build two different models for 0-290 seconds and 300-540 (after near_price is being published) AND don't care about the time dimension\n# Attempt 2: build two different models for 0-60 seconds and 70-540 (after you can obtain target shift(7) without data leakage) AND and do care about the time dimension (remember to iterate over stocks)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:05:52.884993Z","iopub.execute_input":"2023-12-04T09:05:52.885646Z","iopub.status.idle":"2023-12-04T09:05:52.890947Z","shell.execute_reply.started":"2023-12-04T09:05:52.885606Z","shell.execute_reply":"2023-12-04T09:05:52.889665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agg_list = ['sum', 'min', 'max', 'mean', 'median', lambda x: x.mean()/x.std(), pd.Series.kurtosis, pd.Series.skew, 'count']","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:15.567004Z","iopub.execute_input":"2023-11-30T09:14:15.567343Z","iopub.status.idle":"2023-11-30T09:14:15.578097Z","shell.execute_reply.started":"2023-11-30T09:14:15.567314Z","shell.execute_reply":"2023-11-30T09:14:15.576642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(df['target'], width=500, height=500)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:15.579651Z","iopub.execute_input":"2023-11-30T09:14:15.580010Z","iopub.status.idle":"2023-11-30T09:14:18.090402Z","shell.execute_reply.started":"2023-11-30T09:14:15.579980Z","shell.execute_reply":"2023-11-30T09:14:18.089041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['target']].agg(func=agg_list).round(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:18.091976Z","iopub.execute_input":"2023-11-30T09:14:18.092374Z","iopub.status.idle":"2023-11-30T09:14:19.887193Z","shell.execute_reply.started":"2023-11-30T09:14:18.092324Z","shell.execute_reply":"2023-11-30T09:14:19.885908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['target']].describe().round(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:19.889033Z","iopub.execute_input":"2023-11-30T09:14:19.889545Z","iopub.status.idle":"2023-11-30T09:14:20.141619Z","shell.execute_reply.started":"2023-11-30T09:14:19.889508Z","shell.execute_reply":"2023-11-30T09:14:20.140480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['target'].quantile(q=0.95)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:20.143105Z","iopub.execute_input":"2023-11-30T09:14:20.143577Z","iopub.status.idle":"2023-11-30T09:14:20.231944Z","shell.execute_reply.started":"2023-11-30T09:14:20.143543Z","shell.execute_reply":"2023-11-30T09:14:20.230885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['target'].quantile(q=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:20.233224Z","iopub.execute_input":"2023-11-30T09:14:20.233561Z","iopub.status.idle":"2023-11-30T09:14:20.300108Z","shell.execute_reply.started":"2023-11-30T09:14:20.233532Z","shell.execute_reply":"2023-11-30T09:14:20.298650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[df['target'] < df['target'].quantile(q=0.05), 'date_id'].nunique()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:20.301468Z","iopub.execute_input":"2023-11-30T09:14:20.301896Z","iopub.status.idle":"2023-11-30T09:14:20.391158Z","shell.execute_reply.started":"2023-11-30T09:14:20.301864Z","shell.execute_reply":"2023-11-30T09:14:20.390071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tscv = TimeSeriesSplit(n_splits=10)\nkfcv = KFold(n_splits=10)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:20.394871Z","iopub.execute_input":"2023-11-30T09:14:20.395207Z","iopub.status.idle":"2023-11-30T09:14:20.400351Z","shell.execute_reply.started":"2023-11-30T09:14:20.395179Z","shell.execute_reply":"2023-11-30T09:14:20.399182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define features for the Model I\nfeatures_1 = [\n    'imbalance_size_vs_matched_size', 'imbalance_size_vs_matched_size*imbalance_buy_sell_flag', 'bid_ask_spread_relative',\n    'bid_price_vs_reference_price', 'ask_price_vs_reference_price',\n]\n\n# Define features for the Model II\nfeatures_2 = [\n    'imbalance_size_vs_matched_size', 'imbalance_size_vs_matched_size*imbalance_buy_sell_flag', 'bid_ask_spread_relative',\n    'near_price_vs_reference_price', 'far_price_vs_reference_price',\n    'bid_price_vs_reference_price', 'ask_price_vs_reference_price',\n]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:20.401764Z","iopub.execute_input":"2023-11-30T09:14:20.402138Z","iopub.status.idle":"2023-11-30T09:14:20.414197Z","shell.execute_reply.started":"2023-11-30T09:14:20.402109Z","shell.execute_reply":"2023-11-30T09:14:20.413123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_1 = df.loc[df['seconds_in_bucket'] < 300, features_1].copy()\ny_1 = df.loc[df['seconds_in_bucket'] < 300, 'target'].copy()\n\nX_2 = df.loc[df['seconds_in_bucket'] >= 300, features_1].copy()\ny_2 = df.loc[df['seconds_in_bucket'] >= 300, 'target'].copy()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:20.415678Z","iopub.execute_input":"2023-11-30T09:14:20.416003Z","iopub.status.idle":"2023-11-30T09:14:21.681492Z","shell.execute_reply.started":"2023-11-30T09:14:20.415976Z","shell.execute_reply":"2023-11-30T09:14:21.680437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import get_scorer_names","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:21.682778Z","iopub.execute_input":"2023-11-30T09:14:21.683086Z","iopub.status.idle":"2023-11-30T09:14:21.689073Z","shell.execute_reply.started":"2023-11-30T09:14:21.683058Z","shell.execute_reply":"2023-11-30T09:14:21.687422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_scorer_names()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:21.690328Z","iopub.execute_input":"2023-11-30T09:14:21.690661Z","iopub.status.idle":"2023-11-30T09:14:21.701096Z","shell.execute_reply.started":"2023-11-30T09:14:21.690633Z","shell.execute_reply":"2023-11-30T09:14:21.700037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LinearRegression(fit_intercept=True)\n\n# Compute 10-fold TimeSeriesSplit CV scores\ncv_scores = np.round(-cross_val_score(estimator=model, X=X_1, y=y_1, cv=tscv, scoring='neg_mean_absolute_error'), 2)\nprint(f'tscv: {cv_scores} | {np.round(np.mean(cv_scores), 2)}')\n\n# Compute 10-fold KFold CV scores\ncv_scores = np.round(-cross_val_score(estimator=model, X=X_1, y=y_1, cv=kfcv, scoring='neg_mean_absolute_error'), 2)\nprint(f'kfcv: {cv_scores} | {np.round(np.mean(cv_scores), 2)}')","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:21.702321Z","iopub.execute_input":"2023-11-30T09:14:21.702649Z","iopub.status.idle":"2023-11-30T09:14:32.002613Z","shell.execute_reply.started":"2023-11-30T09:14:21.702621Z","shell.execute_reply":"2023-11-30T09:14:32.000932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LinearRegression()\n\n# Compute 10-fold TimeSeriesSplit CV scores\ncv_scores = np.round(-cross_val_score(estimator=model, X=X_2, y=y_2, cv=tscv, scoring='neg_mean_absolute_error'), 2)\nprint(f'tscv: {cv_scores} | {np.round(np.mean(cv_scores), 2)}')\n\n# Compute 10-fold KFold CV scores\ncv_scores = np.round(-cross_val_score(estimator=model, X=X_2, y=y_2, cv=kfcv, scoring='neg_mean_absolute_error'), 2)\nprint(f'kfcv: {cv_scores} | {np.round(np.mean(cv_scores), 2)}')","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:32.004515Z","iopub.execute_input":"2023-11-30T09:14:32.004936Z","iopub.status.idle":"2023-11-30T09:14:40.571592Z","shell.execute_reply.started":"2023-11-30T09:14:32.004894Z","shell.execute_reply":"2023-11-30T09:14:40.570426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Attempt 1: \n* Model I for 0-290 seconds\n* Model II for 300-540 (after near_price is being published)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T14:56:44.842330Z","iopub.execute_input":"2023-11-19T14:56:44.842830Z","iopub.status.idle":"2023-11-19T14:56:44.848812Z","shell.execute_reply.started":"2023-11-19T14:56:44.842797Z","shell.execute_reply":"2023-11-19T14:56:44.847329Z"}}},{"cell_type":"code","source":"def get_kfold_cv_scores_separate(\n    list_models: List = None,\n    data: pd.DataFrame = None,\n    features_1: List[str] = None,\n    features_2: List[str] = None,\n    n_splits: int = 10,\n) -> Dict:\n    \n    # Use dates to make the splits, instead of using rows directly\n    dates = data['date_id'].unique()\n    kfcv = KFold(n_splits=n_splits)\n    PURGE = 1\n \n    # Store performance in a dict\n    stats_CV = {}\n    for train, test in tqdm(kfcv.split(dates)):\n        \n        # Purge\n        if test.min() > train.max():\n            train = train[:-PURGE]\n        elif test.max() < train.min():\n            train = train[PURGE:]\n        elif test.min() > train.min() and test.max() < train.max():\n            train = np.concatenate((train[train < test.min()][:-PURGE], train[train > test.max()][PURGE:]))\n        \n        fold_train = data.loc[data['date_id'].isin(train), :]\n        fold_test = data.loc[data['date_id'].isin(test), :]\n        \n        for model in list_models:\n            \n            model_name = model.__class__.__name__\n            \n            if model_name == 'Lasso':\n                \n                # Standardize features_1 using only the training set (avoid the information leakage)\n                scaler = StandardScaler()\n                scaler.fit(fold_train.loc[fold_train['seconds_in_bucket'] < 300, features_1])\n                \n                # Fit model I and make predictions\n                model.fit(\n                    X=scaler.transform(fold_train.loc[fold_train['seconds_in_bucket'] < 300, features_1]),\n                    y=fold_train.loc[fold_train['seconds_in_bucket'] < 300, 'target']\n                )\n                preds_1 = model.predict(scaler.transform(fold_test.loc[fold_test['seconds_in_bucket'] < 300, features_1]))\n                \n                # Standardize features_2 using only the training set (avoid the information leakage)\n                scaler = StandardScaler()\n                scaler.fit(fold_train.loc[fold_train['seconds_in_bucket'] >= 300, features_2])\n                \n                # Fit model II and make predictions\n                model.fit(\n                    X=scaler.transform(fold_train.loc[fold_train['seconds_in_bucket'] >= 300, features_2]),\n                    y=fold_train.loc[fold_train['seconds_in_bucket'] >= 300, 'target']\n                )\n                preds_2 = model.predict(scaler.transform(fold_test.loc[fold_test['seconds_in_bucket'] >= 300, features_2]))\n            \n            else:\n                \n                # Just fit two models and make predictions\n                model.fit(\n                    X=fold_train.loc[fold_train['seconds_in_bucket'] < 300, features_1],\n                    y=fold_train.loc[fold_train['seconds_in_bucket'] < 300, 'target']\n                )\n                preds_1 = model.predict(fold_test.loc[fold_test['seconds_in_bucket'] < 300, features_1])\n            \n                model.fit(\n                    X=fold_train.loc[fold_train['seconds_in_bucket'] >= 300, features_2],\n                    y=fold_train.loc[fold_train['seconds_in_bucket'] >= 300, 'target']\n                )\n                preds_2 = model.predict(fold_test.loc[fold_test['seconds_in_bucket'] >= 300, features_2])\n            \n            # Combine predictions and targets to compute the metric\n            preds_concat = np.concatenate([preds_1, preds_2])\n            fold_test_y_concat = pd.concat(objs=[\n                fold_test.loc[fold_test['seconds_in_bucket'] < 300, 'target'],\n                fold_test.loc[fold_test['seconds_in_bucket'] >= 300, 'target']\n            ])\n            score = mean_absolute_error(preds_concat, fold_test_y_concat)          \n            \n            if model_name in stats_CV:\n                stats_CV[model_name].append(score)\n            else:\n                stats_CV[model_name] = [score]\n                \n    return stats_CV","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:40.573947Z","iopub.execute_input":"2023-11-30T09:14:40.575672Z","iopub.status.idle":"2023-11-30T09:14:40.650733Z","shell.execute_reply.started":"2023-11-30T09:14:40.575628Z","shell.execute_reply":"2023-11-30T09:14:40.648933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_walk_forward_fixed_origin_cv_scores_separate(\n    list_models: List = None,\n    data: pd.DataFrame = None,\n    features_1: List[str] = None,\n    features_2: List[str] = None,\n    n_splits: int = 10,\n) -> Dict:\n    \n    # Use dates to make the splits, instead of using rows directly\n    dates = data['date_id'].unique()\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    PURGE = 1\n \n    # Store performance in a dict\n    stats_CV = {}\n    for train, test in tqdm(tscv.split(dates)):\n        \n        # Purge 1 date\n        train = train[:-PURGE]\n        \n        fold_train = data.loc[data['date_id'].isin(train), :]\n        fold_test = data.loc[data['date_id'].isin(test), :]\n        \n        for model in list_models:\n            \n            model_name = model.__class__.__name__\n            \n            if model_name == 'Lasso':\n                \n                # Standardize features_1 using only the training set (avoid the information leakage)\n                scaler = StandardScaler()\n                scaler.fit(fold_train.loc[fold_train['seconds_in_bucket'] < 300, features_1])\n                \n                # Fit model I and make predictions\n                model.fit(\n                    X=scaler.transform(fold_train.loc[fold_train['seconds_in_bucket'] < 300, features_1]),\n                    y=fold_train.loc[fold_train['seconds_in_bucket'] < 300, 'target']\n                )\n                preds_1 = model.predict(scaler.transform(fold_test.loc[fold_test['seconds_in_bucket'] < 300, features_1]))\n                \n                # Standardize features_2 using only the training set (avoid the information leakage)\n                scaler = StandardScaler()\n                scaler.fit(fold_train.loc[fold_train['seconds_in_bucket'] >= 300, features_2])\n                \n                # Fit model II and make predictions\n                model.fit(\n                    X=scaler.transform(fold_train.loc[fold_train['seconds_in_bucket'] >= 300, features_2]),\n                    y=fold_train.loc[fold_train['seconds_in_bucket'] >= 300, 'target']\n                )\n                preds_2 = model.predict(scaler.transform(fold_test.loc[fold_test['seconds_in_bucket'] >= 300, features_2]))\n            \n            else:\n                \n                # Just fit two models and make predictions\n                model.fit(\n                    X=fold_train.loc[fold_train['seconds_in_bucket'] < 300, features_1],\n                    y=fold_train.loc[fold_train['seconds_in_bucket'] < 300, 'target']\n                )\n                preds_1 = model.predict(fold_test.loc[fold_test['seconds_in_bucket'] < 300, features_1])\n            \n                model.fit(\n                    X=fold_train.loc[fold_train['seconds_in_bucket'] >= 300, features_2],\n                    y=fold_train.loc[fold_train['seconds_in_bucket'] >= 300, 'target']\n                )\n                preds_2 = model.predict(fold_test.loc[fold_test['seconds_in_bucket'] >= 300, features_2])\n            \n            # Combine predictions and targets to compute the metric\n            preds_concat = np.concatenate([preds_1, preds_2])\n            fold_test_y_concat = pd.concat(objs=[\n                fold_test.loc[fold_test['seconds_in_bucket'] < 300, 'target'],\n                fold_test.loc[fold_test['seconds_in_bucket'] >= 300, 'target']\n            ])\n            score = mean_absolute_error(preds_concat, fold_test_y_concat)          \n            \n            if model_name in stats_CV:\n                stats_CV[model_name].append(score)\n            else:\n                stats_CV[model_name] = [score]\n                \n    return stats_CV","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:40.658683Z","iopub.execute_input":"2023-11-30T09:14:40.663078Z","iopub.status.idle":"2023-11-30T09:14:40.693840Z","shell.execute_reply.started":"2023-11-30T09:14:40.663008Z","shell.execute_reply":"2023-11-30T09:14:40.692865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test if everything is okay\nstatsCV = get_kfold_cv_scores_separate(list_models=[LinearRegression(fit_intercept=True)], data=df, features_1=features_1, features_2=features_2, n_splits=10)\nfor key, item in statsCV.items():\n    print(f'{key} model obtained an average score of {str(np.round(np.mean(item), 3))} in this CV scheme, with a standard deviation of {str(np.round(np.std(item), 3))}')\nfig = px.line(data_frame=statsCV, markers='.')\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=30, pad=1))\nfig.update_xaxes(tickvals=np.arange(start=0, stop=10))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:14:40.695555Z","iopub.execute_input":"2023-11-30T09:14:40.696275Z","iopub.status.idle":"2023-11-30T09:15:12.654101Z","shell.execute_reply.started":"2023-11-30T09:14:40.696188Z","shell.execute_reply":"2023-11-30T09:15:12.652963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test if everything is okay\nstatsCV = get_walk_forward_fixed_origin_cv_scores_separate(list_models=[LinearRegression(fit_intercept=True)], data=df, features_1=features_1, features_2=features_2, n_splits=10)\nfor key, item in statsCV.items():\n    print(f'{key} model obtained an average score of {str(np.round(np.mean(item), 3))} in this CV scheme, with a standard deviation of {str(np.round(np.std(item), 3))}')\nfig = px.line(data_frame=statsCV, markers='.')\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=30, pad=1))\nfig.update_xaxes(tickvals=np.arange(start=0, stop=10))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:15:12.656139Z","iopub.execute_input":"2023-11-30T09:15:12.656967Z","iopub.status.idle":"2023-11-30T09:15:32.695752Z","shell.execute_reply.started":"2023-11-30T09:15:12.656923Z","shell.execute_reply":"2023-11-30T09:15:32.694391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_models = [\n    sklearn.linear_model.LinearRegression(fit_intercept=True), # probably the simplest model possible for this problem \n    sklearn.linear_model.RANSACRegressor( # https://scikit-learn.org/stable/auto_examples/linear_model/plot_ransac.html#sphx-glr-auto-examples-linear-model-plot-ransac-py\n        estimator=None, # if estimator is None, then LinearRegression is used\n        loss='absolute_error',\n        random_state=42,\n    ),\n    sklearn.linear_model.Lasso(\n        alpha=1, # alpha is a constant that multiplies the L1 term, controlling regularization strength. I require maximum penalty.\n        fit_intercept=False, # if set to False, no intercept will be used in calculations (i.e. data is expected to be centered).\n    ),\n    xgb.XGBRegressor( # pre pruning\n        objective='reg:absoluteerror', # regression with L1 error\n        seed=42,\n        max_depth=5,\n        learning_rate=0.3,\n        min_child_weight=100, # minimum sum of instance weight (hessian) needed in a child\n        gamma=1, # minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be. range: [0,∞]\n        n_jobs=-1,\n    ), \n    lgbm.LGBMRegressor( # pre pruning\n        objective=custom_mae,\n        random_state=42,\n        max_depth=5,\n        learning_rate=0.3,\n        min_child_weight=100, # minimum sum of instance weight (hessian) needed in a child\n        min_child_samples=50, # minimum number of data points required in a leaf, it helps to control the minimum size of leaves\n        # max_leaves=31, maximum tree leaves for base learners is 31, so I commented this line to avoid lgbm warnings \n        n_jobs=-1,\n    ),\n]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:15:32.705286Z","iopub.execute_input":"2023-11-30T09:15:32.706501Z","iopub.status.idle":"2023-11-30T09:15:32.721000Z","shell.execute_reply.started":"2023-11-30T09:15:32.706455Z","shell.execute_reply":"2023-11-30T09:15:32.719520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"statsCV = get_kfold_cv_scores_separate(list_models=list_models, data=df, features_1=features_1, features_2=features_2, n_splits=10)\nfor key, item in statsCV.items():\n    print(f'{key} model obtained an average score of {str(np.round(np.mean(item), 3))} in this CV scheme, with a standard deviation of {str(np.round(np.std(item), 3))}')\nfig = px.line(data_frame=statsCV, markers='.')\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=30, pad=1))\nfig.update_xaxes(tickvals=np.arange(start=0, stop=10))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:15:32.722574Z","iopub.execute_input":"2023-11-30T09:15:32.723246Z","iopub.status.idle":"2023-11-30T11:20:44.117404Z","shell.execute_reply.started":"2023-11-30T09:15:32.723189Z","shell.execute_reply":"2023-11-30T11:20:44.116035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"statsCV = get_walk_forward_fixed_origin_cv_scores_separate(list_models=list_models, data=df, features_1=features_1, features_2=features_2, n_splits=10)\nfor key, item in statsCV.items():\n    print(f'{key} model obtained an average score of {str(np.round(np.mean(item), 3))} in this CV scheme, with a standard deviation of {str(np.round(np.std(item), 3))}')\nfig = px.line(data_frame=statsCV, markers='.')\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=30, pad=1))\nfig.update_xaxes(tickvals=np.arange(start=0, stop=10))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:20:44.119574Z","iopub.execute_input":"2023-11-30T11:20:44.120012Z","iopub.status.idle":"2023-11-30T12:28:59.438690Z","shell.execute_reply.started":"2023-11-30T11:20:44.119973Z","shell.execute_reply":"2023-11-30T12:28:59.437296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Attempt 2: \n* One model for 0-540 seconds","metadata":{"execution":{"iopub.status.busy":"2023-11-19T14:56:44.842330Z","iopub.execute_input":"2023-11-19T14:56:44.842830Z","iopub.status.idle":"2023-11-19T14:56:44.848812Z","shell.execute_reply.started":"2023-11-19T14:56:44.842797Z","shell.execute_reply":"2023-11-19T14:56:44.847329Z"}}},{"cell_type":"code","source":"def get_kfold_cv_scores_combined(\n    list_models: List = None,\n    data: pd.DataFrame = None,\n    features: List[str] = None,\n    n_splits: int = 10,\n) -> Dict:\n    \n    # Use dates to make the splits, instead of using rows directly\n    dates = data['date_id'].unique()\n    kfcv = KFold(n_splits=n_splits)\n    PURGE = 1\n \n    # Store performance in a dict\n    stats_CV = {}\n    for train, test in tqdm(kfcv.split(dates)):\n        \n        # Purge\n        if test.min() > train.max():\n            train = train[:-PURGE]\n        elif test.max() < train.min():\n            train = train[PURGE:]\n        elif test.min() > train.min() and test.max() < train.max():\n            train = np.concatenate((train[train < test.min()][:-PURGE], train[train > test.max()][PURGE:]))\n        \n        fold_train = data.loc[data['date_id'].isin(train), :]\n        fold_test = data.loc[data['date_id'].isin(test), :]\n        \n        for model in list_models:\n            \n            model_name = model.__class__.__name__\n            \n            if model_name == 'Lasso':\n                \n                # Standardize features using only the training set (avoid the information leakage)\n                scaler = StandardScaler()\n                scaler.fit(fold_train[features])\n                \n                # Fit the model and make predictions\n                model.fit(\n                    X=scaler.transform(fold_train[features]),\n                    y=fold_train['target']\n                )\n                preds = model.predict(scaler.transform(fold_test[features]))\n            \n            else:\n                \n                # Just fit the model and make predictions\n                model.fit(\n                    X=fold_train[features],\n                    y=fold_train['target']\n                )\n                preds = model.predict(fold_test[features])\n                \n            # Calculate score\n            score = mean_absolute_error(preds, fold_test['target'])          \n            \n            if model_name in stats_CV:\n                stats_CV[model_name].append(score)\n            else:\n                stats_CV[model_name] = [score]\n                \n    return stats_CV","metadata":{"execution":{"iopub.status.busy":"2023-11-30T12:28:59.440277Z","iopub.execute_input":"2023-11-30T12:28:59.440681Z","iopub.status.idle":"2023-11-30T12:28:59.454878Z","shell.execute_reply.started":"2023-11-30T12:28:59.440649Z","shell.execute_reply":"2023-11-30T12:28:59.454009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_walk_forward_fixed_origin_cv_scores_combined(\n    list_models: List = None,\n    data: pd.DataFrame = None,\n    features: List[str] = None,\n    n_splits: int = 10,\n) -> Dict:\n    \n    # Use dates to make the splits, instead of using rows directly\n    dates = data['date_id'].unique()\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    PURGE = 1\n \n    # Store performance in a dict\n    stats_CV = {}\n    for train, test in tqdm(tscv.split(dates)):\n        \n        # Purge 1 date\n        train = train[:-PURGE]\n        \n        fold_train = data.loc[data['date_id'].isin(train), :]\n        fold_test = data.loc[data['date_id'].isin(test), :]\n        \n        for model in list_models:\n            \n            model_name = model.__class__.__name__\n            \n            if model_name == 'Lasso':\n                \n                # Standardize features using only the training set (avoid the information leakage)\n                scaler = StandardScaler()\n                scaler.fit(fold_train[features])\n                \n                # Fit the model and make predictions\n                model.fit(\n                    X=scaler.transform(fold_train[features]),\n                    y=fold_train['target']\n                )\n                preds = model.predict(scaler.transform(fold_test[features]))\n            \n            else:\n                \n                # Just fit the model and make predictions\n                model.fit(\n                    X=fold_train[features],\n                    y=fold_train['target']\n                )\n                preds = model.predict(fold_test[features])\n                \n            # Calculate score\n            score = mean_absolute_error(preds, fold_test['target'])          \n            \n            if model_name in stats_CV:\n                stats_CV[model_name].append(score)\n            else:\n                stats_CV[model_name] = [score]\n                \n    return stats_CV","metadata":{"execution":{"iopub.status.busy":"2023-11-30T12:28:59.456488Z","iopub.execute_input":"2023-11-30T12:28:59.457231Z","iopub.status.idle":"2023-11-30T12:28:59.474272Z","shell.execute_reply.started":"2023-11-30T12:28:59.457198Z","shell.execute_reply":"2023-11-30T12:28:59.473395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define features\nfeatures = [\n    'imbalance_size_vs_matched_size', 'imbalance_size_vs_matched_size*imbalance_buy_sell_flag', 'bid_ask_spread_relative',\n    'near_price_vs_reference_price', 'far_price_vs_reference_price',\n    'bid_price_vs_reference_price', 'ask_price_vs_reference_price',\n]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T12:28:59.475519Z","iopub.execute_input":"2023-11-30T12:28:59.476421Z","iopub.status.idle":"2023-11-30T12:28:59.490435Z","shell.execute_reply.started":"2023-11-30T12:28:59.476390Z","shell.execute_reply":"2023-11-30T12:28:59.489558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"statsCV = get_kfold_cv_scores_combined(list_models=list_models, data=df, features=features, n_splits=10)\nfor key, item in statsCV.items():\n    print(f'{key} model obtained an average score of {str(np.round(np.mean(item), 3))} in this CV scheme, with a standard deviation of {str(np.round(np.std(item), 3))}')\nfig = px.line(data_frame=statsCV, markers='.')\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=30, pad=1))\nfig.update_xaxes(tickvals=np.arange(start=0, stop=10))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T12:28:59.491741Z","iopub.execute_input":"2023-11-30T12:28:59.492255Z","iopub.status.idle":"2023-11-30T13:35:33.526410Z","shell.execute_reply.started":"2023-11-30T12:28:59.492227Z","shell.execute_reply":"2023-11-30T13:35:33.525163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"statsCV = get_walk_forward_fixed_origin_cv_scores_combined(list_models=list_models, data=df, features=features, n_splits=10)\nfor key, item in statsCV.items():\n    print(f'{key} model obtained an average score of {str(np.round(np.mean(item), 3))} in this CV scheme, with a standard deviation of {str(np.round(np.std(item), 3))}')\nfig = px.line(data_frame=statsCV, markers='.')\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=30, pad=1))\nfig.update_xaxes(tickvals=np.arange(start=0, stop=10))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T13:35:33.528253Z","iopub.execute_input":"2023-11-30T13:35:33.529011Z","iopub.status.idle":"2023-11-30T14:37:10.338571Z","shell.execute_reply.started":"2023-11-30T13:35:33.528966Z","shell.execute_reply":"2023-11-30T14:37:10.337473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Attempt 3: \n* One model for 0-540 seconds (default models' params)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T14:56:44.842330Z","iopub.execute_input":"2023-11-19T14:56:44.842830Z","iopub.status.idle":"2023-11-19T14:56:44.848812Z","shell.execute_reply.started":"2023-11-19T14:56:44.842797Z","shell.execute_reply":"2023-11-19T14:56:44.847329Z"}}},{"cell_type":"code","source":"list_models = [\n    sklearn.linear_model.LinearRegression(fit_intercept=True), # probably the simplest model possible for this problem\n    sklearn.linear_model.Lasso(\n        alpha=1, # alpha is a constant that multiplies the L1 term, controlling regularization strength. I require maximum penalty.\n        fit_intercept=False, # If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).\n    ),\n    xgb.XGBRegressor(\n        objective='reg:absoluteerror',\n        seed=42,\n        n_jobs=-1,\n    ), \n    lgbm.LGBMRegressor(\n        objective=custom_mae,\n        random_state=42,\n        n_jobs=-1,\n    ),\n]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T14:37:10.340636Z","iopub.execute_input":"2023-11-30T14:37:10.341005Z","iopub.status.idle":"2023-11-30T14:37:10.351409Z","shell.execute_reply.started":"2023-11-30T14:37:10.340974Z","shell.execute_reply":"2023-11-30T14:37:10.349915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define features\nfeatures = [\n    'imbalance_size_vs_matched_size', 'imbalance_size_vs_matched_size*imbalance_buy_sell_flag', 'bid_ask_spread_relative',\n    'near_price_vs_reference_price', 'far_price_vs_reference_price',\n    'bid_price_vs_reference_price', 'ask_price_vs_reference_price',\n]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T14:37:10.352793Z","iopub.execute_input":"2023-11-30T14:37:10.353168Z","iopub.status.idle":"2023-11-30T14:37:10.366918Z","shell.execute_reply.started":"2023-11-30T14:37:10.353137Z","shell.execute_reply":"2023-11-30T14:37:10.365575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"statsCV = get_kfold_cv_scores_combined(list_models=list_models, data=df, features=features, n_splits=10)\nfor key, item in statsCV.items():\n    print(f'{key} model obtained an average score of {str(np.round(np.mean(item), 3))} in this CV scheme, with a standard deviation of {str(np.round(np.std(item), 3))}')\nfig = px.line(data_frame=statsCV, markers='.')\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=30, pad=1))\nfig.update_xaxes(tickvals=np.arange(start=0, stop=10))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T14:37:10.368343Z","iopub.execute_input":"2023-11-30T14:37:10.368748Z","iopub.status.idle":"2023-11-30T15:48:20.625240Z","shell.execute_reply.started":"2023-11-30T14:37:10.368715Z","shell.execute_reply":"2023-11-30T15:48:20.623648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"statsCV = get_walk_forward_fixed_origin_cv_scores_combined(list_models=list_models, data=df, features=features, n_splits=10)\nfor key, item in statsCV.items():\n    print(f'{key} model obtained an average score of {str(np.round(np.mean(item), 3))} in this CV scheme, with a standard deviation of {str(np.round(np.std(item), 3))}')\nfig = px.line(data_frame=statsCV, markers='.')\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=30, pad=1))\nfig.update_xaxes(tickvals=np.arange(start=0, stop=10))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T15:48:20.626737Z","iopub.execute_input":"2023-11-30T15:48:20.627093Z","iopub.status.idle":"2023-11-30T17:02:08.599543Z","shell.execute_reply.started":"2023-11-30T15:48:20.627062Z","shell.execute_reply":"2023-11-30T17:02:08.597687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Attempt 4: \n* One model for 0-540 seconds (default models' params) AND objective function is not MAE this time","metadata":{"execution":{"iopub.status.busy":"2023-11-19T14:56:44.842330Z","iopub.execute_input":"2023-11-19T14:56:44.842830Z","iopub.status.idle":"2023-11-19T14:56:44.848812Z","shell.execute_reply.started":"2023-11-19T14:56:44.842797Z","shell.execute_reply":"2023-11-19T14:56:44.847329Z"}}},{"cell_type":"code","source":"list_models = [\n    sklearn.linear_model.LinearRegression(fit_intercept=True), # probably the simplest model possible for this problem\n    sklearn.linear_model.Lasso(\n        alpha=1, # alpha is a constant that multiplies the L1 term, controlling regularization strength. I require maximum penalty.\n        fit_intercept=False, # If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).\n    ),\n    xgb.XGBRegressor(\n        seed=42,\n        n_jobs=-1,\n    ), \n    lgbm.LGBMRegressor(\n        random_state=42,\n        n_jobs=-1,\n    ),\n]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:02:08.601013Z","iopub.execute_input":"2023-11-30T17:02:08.601388Z","iopub.status.idle":"2023-11-30T17:02:08.619336Z","shell.execute_reply.started":"2023-11-30T17:02:08.601340Z","shell.execute_reply":"2023-11-30T17:02:08.617324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define features\nfeatures = [\n    'imbalance_size_vs_matched_size', 'imbalance_size_vs_matched_size*imbalance_buy_sell_flag', 'bid_ask_spread_relative',\n    'near_price_vs_reference_price', 'far_price_vs_reference_price',\n    'bid_price_vs_reference_price', 'ask_price_vs_reference_price',\n]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:02:08.621416Z","iopub.execute_input":"2023-11-30T17:02:08.621840Z","iopub.status.idle":"2023-11-30T17:02:08.628146Z","shell.execute_reply.started":"2023-11-30T17:02:08.621805Z","shell.execute_reply":"2023-11-30T17:02:08.627220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"statsCV = get_kfold_cv_scores_combined(list_models=list_models, data=df, features=features, n_splits=10)\nfor key, item in statsCV.items():\n    print(f'{key} model obtained an average score of {str(np.round(np.mean(item), 3))} in this CV scheme, with a standard deviation of {str(np.round(np.std(item), 3))}')\nfig = px.line(data_frame=statsCV, markers='.')\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=30, pad=1))\nfig.update_xaxes(tickvals=np.arange(start=0, stop=10))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:02:08.629145Z","iopub.execute_input":"2023-11-30T17:02:08.629524Z","iopub.status.idle":"2023-11-30T17:21:33.932172Z","shell.execute_reply.started":"2023-11-30T17:02:08.629491Z","shell.execute_reply":"2023-11-30T17:21:33.931069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"statsCV = get_walk_forward_fixed_origin_cv_scores_combined(list_models=list_models, data=df, features=features, n_splits=10)\nfor key, item in statsCV.items():\n    print(f'{key} model obtained an average score of {str(np.round(np.mean(item), 3))} in this CV scheme, with a standard deviation of {str(np.round(np.std(item), 3))}')\nfig = px.line(data_frame=statsCV, markers='.')\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=30, pad=1))\nfig.update_xaxes(tickvals=np.arange(start=0, stop=10))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T17:21:33.933575Z","iopub.execute_input":"2023-11-30T17:21:33.934162Z","iopub.status.idle":"2023-11-30T18:18:30.756563Z","shell.execute_reply.started":"2023-11-30T17:21:33.934127Z","shell.execute_reply":"2023-11-30T18:18:30.755743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Key takeaways:\n    * Near price is remarkably important (models for 0-290 seconds have MAE of ~7, whereas models for 300-540 seconds have MAE of ~below 6) regardless of kind of cross-validation scheme.\n    * There's no significant difference between having two separate models for different periods of time and having a single model.\n    * RANSAC model is terribly bad.\n    * Other models perform amazingly similar to each other, even without prepruning. This push me to select the OLS model for final submission.\n    * There's no significant difference between average scores of KFold CV and Walk Forward Time Series CV, thus relearning is not necessary.\n    * In sum, I will train the OLS model with full train data and won't relearn it with new test data during the submission phase.","metadata":{}}]}